Title: Core Agent Classes - LlamaIndex

URL Source: https://docs.llamaindex.ai/en/stable/api_reference/agent/

Markdown Content:
Skip to content
LlamaIndex
Core Agent Classes
Initializing search
LlamaIndex
Home
Home
High-Level Concepts
Installation and Setup
How to read these docs
Starter Examples
Starter Examples
Starter Tutorial (OpenAI)
Starter Tutorial (Local Models)
Discover LlamaIndex Video Series
Frequently Asked Questions (FAQ)
Starter Tools
Starter Tools
RAG CLI
Learn
Learn
Using LLMs
Building a RAG pipeline
Building a RAG pipeline
Loading & Ingestion
Loading & Ingestion
Loading Data (Ingestion)
LlamaHub
Indexing & Embedding
Storing
Querying
Building an agent
Building an agent
Building a basic agent
Agents with local models
Adding RAG to an agent
Enhancing with LlamaParse
Memory
Adding other tools
Tracing and Debugging
Evaluating
Evaluating
Evaluating
Cost Analysis
Cost Analysis
Usage Pattern
Putting it all Together
Putting it all Together
Full-stack web application
Full-stack web application
A Guide to Building a Full-Stack Web App with LLamaIndex
A Guide to Building a Full-Stack LlamaIndex Web App with Delphic
Q&A Patterns
Q&A Patterns
A Guide to Extracting Terms and Definitions
Chatbots
Chatbots
How to Build a Chatbot
Structured data
Structured data
Agents
Use Cases
Use Cases
Prompting
Question-Answering (RAG)
Chatbots
Structured Data Extraction
Agents
Multi-Modal Applications
Fine-Tuning
Examples
Examples
Agents
Agents
ðŸ’¬ðŸ¤– How to Build a Chatbot
GPT Builder Demo
Building a Multi-PDF Agent using Query Pipelines and HyDE
Step-wise, Controllable Agents
Controllable Agents for RAG
Building an Agent around a Query Pipeline
Agentic rag using vertex ai
Agentic rag with llamaindex and vertexai managed index
Function Calling Anthropic Agent
Function Calling AWS Bedrock Converse Agent
Chain-of-Abstraction LlamaPack
Building a Custom Agent
DashScope Agent Tutorial
Introspective Agents: Performing Tasks With Reflection
Language Agent Tree Search
LLM Compiler Agent Cookbook
Simple Composable Memory
Vector Memory
Function Calling Mistral Agent
Multi-Document Agents (V1)
Multi-Document Agents
Build your own OpenAI Agent
Context-Augmented OpenAI Agent
OpenAI Agent Workarounds for Lengthy Tool Descriptions
Single-Turn Multi-Function Calling OpenAI Agents
OpenAI Agent + Query Engine Experimental Cookbook
OpenAI Agent Query Planning
Retrieval-Augmented OpenAI Agent
OpenAI Agent with Tool Call Parser
OpenAI Agent with Query Engine Tools
OpenAI Assistant Agent
OpenAI Assistant Advanced Retrieval Cookbook
OpenAI agent: specifying a forced function call
Benchmarking OpenAI Retrieval API (through Assistant Agent)
ReAct Agent - A Simple Intro with Calculator Tools
ReAct Agent with Query Engine (RAG) Tools
Controlling Agent Reasoning Loop with Return Direct Tools
Structured Planning Agent
Callbacks
Callbacks
Aim Callback
HoneyHive LlamaIndex Tracer
Langfuse Callback Handler
Llama Debug Handler
OpenInference Callback Handler + Arize Phoenix
Observability with OpenLLMetry
PromptLayer Handler
Token Counting Handler
UpTrain Callback Handler
Wandb Callback Handler
Chat Engines
Chat Engines
Chat Engine - Best Mode
Chat Engine - Condense Plus Context Mode
Chat Engine - Condense Question Mode
Chat Engine - Context Mode
Chat Engine - OpenAI Agent Mode
Chat Engine with a Personality âœ¨
Chat Engine - ReAct Agent Mode
Chat Engine - Simple Mode REPL
Cookbooks
Cookbooks
Anthropic Haiku Cookbook
Codestral from MistralAI Cookbook
Cohere init8 and binary Embeddings Retrieval Evaluation
CrewAI + LlamaIndex Cookbook
Llama3 Cookbook
Llama3 Cookbook with Groq
Llama3 Cookbook with Ollama and Replicate
MistralAI Cookbook
mixedbread Rerank Cookbook
Prometheus-2 Cookbook
Customization
Customization
Azure OpenAI
ChatGPT
HuggingFace LLM - Camel-5b
HuggingFace LLM - StableLM
Chat Prompts Customization
Completion Prompts Customization
Streaming
Streaming for Chat Engine - Condense Question Mode
Data Connectors
Data Connectors
Chroma Reader
DashVector Reader
Database Reader
DeepLake Reader
Discord Reader
Faiss Reader
Github Repo Reader
Google Chat Reader Test
Google Docs Reader
Google Drive Reader
Google Maps Text Search Reader
Google Sheets Reader
Make Reader
Mbox Reader
MilvusReader
MongoDB Reader
MyScale Reader
Notion Reader
Obsidian Reader
Pathway Reader
Pinecone Reader
Psychic Reader
Qdrant Reader
Slack Reader
Twitter Reader
Weaviate Reader
Web Page Reader
Deplot Reader Demo
HTML Tag Reader
Simple Directory Reader
Parallel Processing SimpleDirectoryReader
Simple Directory Reader over a Remote FileSystem
Discover LlamaIndex
Discover LlamaIndex
Discord Thread Management
Docstores
Docstores
Demo: Azure Table Storage as a Docstore
Docstore Demo
Dynamo DB Docstore Demo
Firestore Demo
MongoDB Demo
Redis Docstore+Index Store Demo
Embeddings
Embeddings
Anyscale Embeddings
LangChain Embeddings
OpenAI Embeddings
Aleph Alpha Embeddings
Bedrock Embeddings
Embeddings with Clarifai
Cloudflare Workers AI Embeddings
CohereAI Embeddings
Custom Embeddings
Dashscope embeddings
Databricks Embeddings
Deepinfra
Elasticsearch Embeddings
Qdrant FastEmbed Embeddings
Fireworks Embeddings
Google Gemini Embeddings
Google PaLM Embeddings
Gradient Embeddings
Local Embeddings with HuggingFace
IBM watsonx.ai
Local Embeddings with IPEX-LLM on Intel CPU
Local Embeddings with IPEX-LLM on Intel GPU
Optimized BGE Embedding Model using IntelÂ® Extension for Transformers
Jina 8K Context Window Embeddings
Jina Embeddings
Llamafile Embeddings
LLMRails Embeddings
MistralAI Embeddings
Mixedbread AI Embeddings
Nomic Embedding
NVIDIA NIMs
Oracle Cloud Infrastructure Generative AI
OctoAI Embeddings
Ollama Embeddings
Local Embeddings with OpenVINO
Optimized Embedding Model using Optimum-Intel
PremAI Embeddings
Interacting with Embeddings deployed in Amazon SageMaker Endpoint with LlamaIndex
Text Embedding Inference
Together AI Embeddings
Upstage Embeddings
Voyage Embeddings
Evaluation
Evaluation
BEIR Out of Domain Benchmark
ðŸš€ RAG/LLM Evaluators - DeepEval
HotpotQADistractor Demo
QuestionGeneration
Self Correcting Query Engines - Evaluation & Retry
Tonic Validate Evaluators
How to use UpTrain with LlamaIndex
Answer Relevancy and Context Relevancy Evaluations
BatchEvalRunner - Running Multiple Evaluations
Correctness Evaluator
Faithfulness Evaluator
Guideline Evaluator
Benchmarking LLM Evaluators On The MT-Bench Human Judgement LabelledPairwiseEvaluatorDataset
Benchmarking LLM Evaluators On A Mini MT-Bench (Single Grading) LabelledEvaluatorDataset
Evaluating Multi-Modal RAG
Pairwise Evaluator
Evaluation using Prometheus model
Relevancy Evaluator
Retrieval Evaluation
Embedding Similarity Evaluator
Finetuning
Finetuning
How to Finetune a cross-encoder using LLamaIndex
Finetune Embeddings
Finetuning an Adapter on Top of any Black-Box Embedding Model
Fine Tuning Nous-Hermes-2 With Gradient and LlamaIndex
Fine Tuning Llama2 for Better Structured Outputs With Gradient and LlamaIndex
Fine Tuning for Text-to-SQL With Gradient and LlamaIndex
Knowledge Distillation For Fine-Tuning A GPT-3.5 Judge (Correctness)
Knowledge Distillation For Fine-Tuning A GPT-3.5 Judge (Pairwise)
Fine Tuning MistralAI models using Finetuning API
Fine Tuning GPT-3.5-Turbo
Fine Tuning with Function Calling
Fine-tuning a gpt-3.5 ReAct Agent on Better Chain of Thought
Custom Cohere Reranker
Router Fine-tuning
Ingestion
Ingestion
Advanced Ingestion Pipeline
Async Ingestion Pipeline + Metadata Extraction
Ingestion Pipeline + Document Management
Building a Live RAG Pipeline over Google Drive Files
Parallelizing Ingestion Pipeline
Redis Ingestion Pipeline
LLMs
LLMs
AI21
Aleph Alpha
Anthropic
Anyscale
Azure OpenAI
Bedrock
Bedrock Converse
Clarifai LLM
Cleanlab Trustworthy Language Model
Cohere
DashScope LLMS
DataBricks
DeepInfra
EverlyAI
Fireworks
Fireworks Function Calling Cookbook
Friendli
Gemini
Gradient Base Model
Gradient Model Adapter
Groq
Hugging Face LLMs
IBM watsonx.ai
IPEX-LLM on Intel CPU
IPEX-LLM on Intel GPU
Konko
Langchain
LiteLLM
Replicate - Llama 2 13B
LlamaCPP
ðŸ¦™ x ðŸ¦™ Rap Battle
Llama API
llamafile
LLM Predictor
LM Studio
LocalAI
Maritalk
MistralRS LLM
MistralAI
None
ModelScope LLMS
Monster API <> LLamaIndex
MyMagic AI LLM
Neutrino AI
NVIDIA NIMs
NVIDIA NIMs
Nvidia TensorRT-LLM
Nvidia Triton
Oracle Cloud Infrastructure Generative AI
OctoAI
Ollama - Llama 3
Ollama - Gemma
OpenAI
OpenAI JSON Mode vs. Function Calling for Data Extraction
OpenLLM
OpenRouter
OpenVINO LLMs
Optimum Intel LLMs optimized with IPEX backend
PaLM
Perplexity
Portkey
Predibase
PremAI LlamaIndex
Client of Baidu Intelligent Cloud's Qianfan LLM Platform
RunGPT
Interacting with LLM deployed in Amazon SageMaker Endpoint with LlamaIndex
Solar LLM
Together AI LLM
Unify
Upstage
Vertex AI
Replicate - Vicuna 13B
vLLM
Xorbits Inference
Yi
Llama Datasets
Llama Datasets
Downloading a LlamaDataset from LlamaHub
Benchmarking RAG Pipelines With A LabelledRagDatatset
LlamaDataset Submission Template Notebook
Contributing a LlamaDataset To LlamaHub
Llama Hub
Llama Hub
LlamaHub Demostration
Ollama Llama Pack Example
Llama Pack - Resume Screener ðŸ“„
Llama Packs Example
Low Level
Low Level
Building Evaluation from Scratch
Building an Advanced Fusion Retriever from Scratch
Building Data Ingestion from Scratch
Building RAG from Scratch (Open-source only!)
Building Response Synthesis from Scratch
Building Retrieval from Scratch
Building a Router from Scratch
Building a (Very Simple) Vector Store from Scratch
Managed Indexes
Managed Indexes
Google Generative Language Semantic Retriever
PostgresML Managed Index
Google Cloud LlamaIndex on Vertex AI for RAG
Semantic Retriever Benchmark
Vectara Managed Index
Managed Index with Zilliz Cloud Pipelines
Metadata Extractors
Metadata Extractors
Entity Metadata Extraction
Metadata Extraction and Augmentation w/ Marvin
Extracting Metadata for Better Document Indexing and Understanding
Automated Metadata Extraction for Better Retrieval + Synthesis
Pydantic Extractor
Multi-Modal
Multi-Modal
Chroma Multi-Modal Demo with LlamaIndex
Multi-Modal LLM using Anthropic model for image reasoning
Multi-Modal LLM using Azure OpenAI GPT-4V model for image reasoning
Multi-Modal LLM using DashScope qwen-vl model for image reasoning
Multi-Modal LLM using Google's Gemini model for image understanding and build Retrieval Augmented Generation with LlamaIndex
Multimodal Structured Outputs: GPT-4o vs. Other GPT-4 Variants
GPT4-V Experiments with General, Specific questions and Chain Of Thought (COT) Prompting Technique.
Advanced Multi-Modal Retrieval using GPT4V and Multi-Modal Index/Retriever
Image to Image Retrieval using CLIP embedding and image correlation reasoning using GPT4V
LlaVa Demo with LlamaIndex
Retrieval-Augmented Image Captioning
[Beta] Multi-modal ReAct Agent
Multi-Modal GPT4V Pydantic Program
Multi-Modal RAG using Nomic Embed and Anthropic.
Multi-Modal Retrieval using GPT text embedding and CLIP image embedding for Wikipedia Articles
Multimodal RAG for processing videos using OpenAI GPT4V and LanceDB vectorstore
Multimodal Ollama Cookbook
Multi-Modal LLM using OpenAI GPT-4V model for image reasoning
Multi-Modal LLM using Replicate LlaVa, Fuyu 8B, MiniGPT4 models for image reasoning
Semi-structured Image Retrieval
Multi-Tenancy
Multi-Tenancy
Multi-Tenancy RAG with LlamaIndex
Node Parsers & Text Splitters
Node Parsers & Text Splitters
Semantic Chunker
Semantic double merging chunking
Node Postprocessors
Node Postprocessors
Cohere Rerank
Colbert Rerank
File Based Node Parsers
FlagEmbeddingReranker
Jina Rerank
LLM Reranker Demonstration (Great Gatsby)
LLM Reranker Demonstration (2021 Lyft 10-k)
LongContextReorder
Metadata Replacement + Node Sentence Window
Mixedbread AI Rerank
NVIDIA NIMs
Sentence Embedding OptimizerThis postprocessor optimizes token usage by removing sentences that are not relevant to the query (this is done using embeddings).The percentile cutoff is a measure for using the top percentage of relevant sentences. The threshold cutoff can be specified instead, which uses a raw similarity cutoff for picking which sentences to keep.
PII Masking
Forward/Backward Augmentation
Recency Filtering
SentenceTransformerRerank
Time-Weighted Rerank
VoyageAI Rerank
OpenVINO Rerank
RankGPT Reranker Demonstration (Van Gogh Wiki)
RankLLM Reranker Demonstration (Van Gogh Wiki)
Object Stores
Object Stores
The ObjectIndex Class
Output Parsers
Output Parsers
Guardrails Output Parsing
Langchain Output Parsing
DataFrame Structured Data Extraction
Evaporate Demo
Function Calling Program for Structured Extraction
Guidance Pydantic Program
Guidance for Sub-Question Query Engine
LLM Pydantic Program
LM Format Enforcer Pydantic Program
LM Format Enforcer Regular Expression Generation
OpenAI Pydantic Program
OpenAI function calling for Sub-Question Query Engine
Param Optimizer
Param Optimizer
[WIP] Hyperparameter Optimization for RAG
Prompts
Prompts
Advanced Prompt Techniques (Variable Mappings, Functions)
EmotionPrompt in RAG
Accessing/Customizing Prompts within Higher-Level Modules
"Optimization by Prompting" for RAG
Prompt Engineering for RAG
Property Graph
Property Graph
Using a Property Graph Store
Property Graph Construction with Predefined Schemas
Property Graph Index
Defining a Custom Property Graph Retriever
Neo4j Property Graph Index
Query Engines
Query Engines
Retriever Query Engine with Custom Retrievers - Simple Hybrid Search
JSONalyze Query Engine
Joint QA Summary Query Engine
Retriever Router Query Engine
Router Query Engine
SQL Auto Vector Query Engine
SQL Join Query Engine
SQL Router Query Engine
CitationQueryEngine
Cogniswitch query engine
Defining a Custom Query Engine
Ensemble Query Engine Guide
FLARE Query Engine
JSON Query Engine
Knowledge Graph Query Engine
Knowledge Graph RAG Query Engine
Structured Hierarchical Retrieval
Pandas Query Engine
Recursive Retriever + Query Engine Demo
[Beta] Text-to-SQL with PGVector
Query Engine with Pydantic Outputs
Recursive Retriever + Document Agents
Joint Tabular/Semantic QA over Tesla 10K
Sub Question Query Engine
Query Pipeline
Query Pipeline
An Introduction to LlamaIndex Query Pipelines
Query Pipeline with Async/Parallel Execution
Query Pipeline Chat Engine
Query Pipeline over Pandas DataFrames
Query Pipeline with Routing
Query Pipeline for Advanced Text-to-SQL
Query Transformations
Query Transformations
HyDE Query Transform
Multi-Step Query Engine
Query Transform Cookbook
Response Synthesizers
Response Synthesizers
Pydantic Tree Summarize
Stress-Testing Long Context LLMs with a Recall Task
Pydantic Tree Summarize
Refine
Refine with Structured Answer Filtering
Tree Summarize
Retrievers
Retrievers
Auto Merging Retriever
Comparing Methods for Structured Retrieval (Auto-Retrieval vs. Recursive Retrieval)
Bedrock (Knowledge Bases)
BM25 Retriever
Composable Objects
Activeloop Deep Memory
Ensemble Retrieval Guide
Chunk + Document Hybrid Retrieval with Long-Context Embeddings (Together.ai)
Pathway Retriever
Reciprocal Rerank Fusion Retriever
Recursive Retriever + Node References + Braintrust
Recursive Retriever + Node References
Relative Score Fusion and Distribution-Based Score Fusion
Router Retriever
Simple Fusion Retriever
Auto-Retrieval from a Vectara Index
VideoDB Retriever
You.com Retriever
Tools
Tools
OnDemandLoaderTool Tutorial
Azure Code Interpreter Tool Spec
Cassandra Database Tools
Evaluation Query Engine Tool
Transforms
Transforms
Transforms Evaluation
Use Cases
Use Cases
10K Analysis
10Q Analysis
Email Data Extraction
Github Issue Analysis
Vector Stores
Vector Stores
AWSDocDBDemo
Alibaba Cloud OpenSearch Vector Store
Amazon Neptune - Neptune Analytics vector store
AnalyticDB
Astra DB
Simple Vector Store - Async Index Creation
Awadb Vector Store
Azure AI Search
Azure CosmosDB MongoDB Vector Store
Bagel Vector Store
Bagel Network
Baidu VectorDB
Cassandra Vector Store
Chroma + Fireworks + Nomic with Matryoshka embedding
Chroma
ClickHouse Vector Store
CouchbaseVectorStoreDemo
DashVector Vector Store
Databricks Vector Search
Deep Lake Vector Store Quickstart
DocArray Hnsw Vector Store
DocArray InMemory Vector Store
DuckDB
Elasticsearch Vector Store
Elasticsearch
Epsilla Vector Store
Faiss Vector Store
Firestore Vector Store
Hologres
Jaguar Vector Store
Advanced RAG with temporal filters using LlamaIndex and KDB.AI vector store
LanceDB Vector Store
Lantern Vector Store (auto-retriever)
Lantern Vector Store
Metal Vector Store
Milvus Vector Store With Hybrid Retrieval
Milvus Vector Store
MilvusOperatorFunctionDemo
MongoDBAtlasVectorSearch
now make sure you create the search index with the right name here
MongoDBAtlasVectorSearchRAGOpenAI
MyScale Vector Store
Neo4j vector store
Opensearch Vector Store
pgvecto.rs
Pinecone Vector Store - Hybrid Search
Pinecone Vector Store
Qdrant Vector Store
Qdrant Vector Store - Metadata Filter
Qdrant Vector Store - Default Qdrant Filters
Redis Vector Store
Relyt
Rockset Vector Store
Simple Vector Store
Local Llama2 + VectorStoreIndex
Llama2 + VectorStoreIndex
Simple Vector Stores - Maximum Marginal Relevance Retrieval
S3/R2 Storage
Supabase Vector Store
Tair Vector Store
Tencent Cloud VectorDB
TiDB Vector Store
Timescale Vector Store (PostgreSQL)
txtai Vector Store
Typesense Vector Store
Upstash Vector Store
VearchDemo
Google Vertex AI Vector Search
Vespa Vector Store demo
Weaviate Vector Store - Hybrid Search
Weaviate Vector Store
Auto-Retrieval from a Weaviate Vector Database
Weaviate Vector Store Metadata Filter
Zep Vector Store
Auto-Retrieval from a Vector Database
Chroma Vector Store
Auto-Retrieval from a Vector Database
Guide: Using Vector Store Index with Existing Pinecone Vector Store
Guide: Using Vector Store Index with Existing Weaviate Vector Store
Neo4j Vector Store - Metadata Filter
A Simple to Advanced Guide with Auto-Retrieval (with Pinecone + Arize Phoenix)
Pinecone Vector Store - Metadata Filter
Postgres Vector Store
Hybrid Search with Qdrant BM42
Qdrant Hybrid Search
Component Guides
Component Guides
Models
Models
LLMs
LLMs
Using LLMs
Standalone Usage
Customizing LLMs
Available LLM Integrations
Embeddings
Multi Modal
Prompts
Prompts
Usage pattern
Loading
Loading
Documents and Nodes
Documents and Nodes
Using Documents
Using Nodes
Metadata Extraction
SimpleDirectoryReader
Data Connectors
Data Connectors
Usage Pattern
LlamaParse
Module Guides
Node Parsers / Text Splitters
Node Parsers / Text Splitters
Node Parser Modules
Ingestion Pipeline
Ingestion Pipeline
Transformations
Indexing
Indexing
Index Guide
Vector Store Index
Property Graph Index
Document Management
LlamaCloud
Metadata Extraction
Modules
Storing
Storing
Vector Stores
Document Stores
Index Stores
Chat Stores
Key-Value Stores
Persisting & Loading Data
Customizing Storage
Querying
Querying
Query Engines
Query Engines
Usage Pattern
Response Modes
Streaming
Module Guides
Supporting Modules
Chat Engines
Chat Engines
Usage Pattern
Module Guides
Retrieval
Retrieval
Retriever Modules
Retriever Modes
Node Postprocessors
Node Postprocessors
Node Postprocessor Modules
Response Synthesis
Response Synthesis
Response Synthesis Modules
Routing
Query Pipelines
Query Pipelines
Usage Pattern
Module Guides
Module Usage
Structured Outputs
Structured Outputs
Output Parsing Modules
Query Engines + Pydantic Outputs
Pydantic Program
Agents
Agents
Usage Pattern
Lower-Level Agent API
Module Guides
Tools
Evaluation
Evaluation
Usage Pattern (Response Evaluation)
Usage Pattern (Retrieval)
Modules
LlamaDatasets
LlamaDatasets
Contributing A LabelledRagDataset
Evaluating With LabelledRagDataset's
Evaluating Evaluators with LabelledEvaluatorDataset's
Observability
Observability
Instrumentation
Settings
Advanced Topics
Advanced Topics
Building Performant RAG Applications for Production
Basic Strategies
Agentic strategies
Retrieval
Retrieval
Advanced Retrieval Strategies
Query Transformations
Evaluation
Evaluation
Component Wise Evaluation
End-to-End Evaluation
Evaluation
Fine-Tuning
Writing Custom Modules
Building RAG from Scratch (Lower-Level)
API Reference
API Reference
Agents
Agents
Coa
Dashscope
Introspective
Lats
Llm compiler
Openai
Openai legacy
React
Callbacks
Callbacks
Agentops
Aim
Argilla
Arize phoenix
Deepeval
Honeyhive
Langfuse
Llama debug
Openinference
Promptlayer
Token counter
Uptrain
Wandb
Chat Engines
Chat Engines
Condense plus context
Condense question
Context
Simple
Embeddings
Embeddings
Adapter
Alephalpha
Anyscale
Azure openai
Bedrock
Clarifai
Clip
Cloudflare workersai
Cohere
Dashscope
Databricks
Deepinfra
Elasticsearch
Fastembed
Fireworks
Gemini
Google
Gradient
Huggingface
Huggingface api
Huggingface itrex
Huggingface openvino
Huggingface optimum
Huggingface optimum intel
Ibm
Instructor
Ipex llm
Jinaai
Langchain
Litellm
Llamafile
Llm rails
Mistralai
Mixedbreadai
Nomic
Nvidia
Oci genai
Octoai
Ollama
Openai
Premai
Sagemaker endpoint
Text embeddings inference
Together
Upstage
Vertex
Voyageai
Evaluation
Evaluation
Answer relevancy
Context relevancy
Correctness
Dataset generation
Faithfullness
Guideline
Metrics
Multi modal
Pairwise comparison
Query response
Response
Retrieval
Semantic similarity
Tonic validate
Indexes
Indexes
Colbert
Dashscope
Document summary
Google
Keyword
Knowledge graph
Llama cloud
Postgresml
Property graph
Summary
Tree
Vectara
Vector
Vertexai
Zilliz
Ingestion
Ingestion
Instrumentation
Instrumentation
Event handlers
Event types
Span handlers
Span types
LLMs
LLMs
None
Ai21
Alephalpha
Anthropic
Anyscale
Azure openai
Bedrock
Bedrock converse
Clarifai
Cleanlab
Cohere
Custom llm
Dashscope
Databricks
Deepinfra
Everlyai
Fireworks
Friendli
Gemini
Gradient
Groq
Huggingface
Huggingface api
Ibm
Ipex llm
Konko
Langchain
Litellm
Llama api
Llama cpp
Llamafile
Lmstudio
Localai
Maritalk
Mistral rs
Mistralai
Mlx
Modelscope
Monsterapi
Mymagic
Neutrino
Nvidia
Nvidia tensorrt
Nvidia triton
Oci genai
Octoai
Ollama
Openai
Openai like
Openllm
Openrouter
Openvino
Optimum intel
Palm
Perplexity
Portkey
Predibase
Premai
Qianfan
Replicate
Rungpt
Sagemaker endpoint
Solar
Text generation inference
Together
Unify
Upstage
Vertex
Vllm
Xinference
Yi
You
Llama Datasets
Llama Datasets
Llama Packs
Llama Packs
Agent search retriever
Agents coa
Agents lats
Agents llm compiler
Amazon product extraction
Arize phoenix query engine
Auto merging retriever
Chroma autoretrieval
Code hierarchy
Cogniswitch agent
Cohere citation chat
Corrective rag
Deeplake deepmemory retriever
Deeplake multimodal retrieval
Dense x retrieval
Diff private simple dataset
Docugami kg rag
Evaluator benchmarker
Finchat
Fusion retriever
Fuzzy citation
Gmail openai agent
Gradio agent chat
Gradio react agent chatbot
Infer retrieve rerank
Koda retriever
Llama dataset metadata
Llama guard moderator
Llava completion
Mixture of agents
Multi document agents
Multi tenancy rag
Multidoc autoretrieval
Nebulagraph query engine
Neo4j query engine
Node parser semantic chunking
Ollama query engine
Panel chatbot
Query understanding agent
Raft dataset
Rag cli local
Rag evaluator
Rag fusion query pipeline
Ragatouille retriever
Raptor
Recursive retriever
Redis ingestion pipeline
Resume screener
Retry engine weaviate
Searchain
Secgpt
Self discover
Self rag
Sentence window retriever
Snowflake query engine
Stock market data query engine
Streamlit chatbot
Sub question weaviate
Subdoc summary
Tables
Timescale vector autoretrieval
Trulens eval packs
Vanna
Vectara rag
Voyage query engine
Zenguard
Zephyr query engine
Memory
Memory
Chat memory buffer
Simple composable memory
Vector memory
Metadata Extractors
Metadata Extractors
Entity
Keyword
Marvin
Pydantic
Question
Summary
Title
Multi-Modal LLMs
Multi-Modal LLMs
Anthropic
Azure openai
Dashscope
Gemini
Ollama
Openai
Replicate
Node Parsers & Text Splitters
Node Parsers & Text Splitters
Dashscope
Code
Hierarchical
Html
Json
Langchain
Markdown
Markdown element
Semantic splitter
Sentence splitter
Sentence window
Token text splitter
Unstructured element
Node Postprocessors
Node Postprocessors
NER PII
PII
Auto prev next
Cohere rerank
Colbert rerank
Dashscope rerank
Embedding recency
Fixed recency
Flag embedding reranker
Jinaai rerank
Keyword
Llm rerank
Long context reorder
Longllmlingua
Metadata replacement
Mixedbreadai rerank
Nvidia rerank
Openvino rerank
Presidio
Prev next
Rankgpt rerank
Rankllm rerank
Sbert rerank
Sentence optimizer
Similarity
Time weighted
Voyageai rerank
Object Stores
Object Stores
Output Parsers
Output Parsers
Guardrails
Langchain
Pydantic
Selection
Programs
Programs
Evaporate
Guidance
Llm text completion
Lmformatenforcer
Multi modal
Openai
Prompts
Prompts
Query Engines
Query Engines
FLARE
JSONalayze
NL SQL table
PGVector SQL
SQL join
SQL table retriever
Citation
Cogniswitch
Custom
Knowledge graph
Multi step
Pandas
Retriever
Retriever router
Retry
Router
Simple multi modal
Sub question
Tool retriever router
Transform
Query Pipeline
Query Pipeline
Agent
Arg pack
Custom
Function
Input
Llm
Multi modal
Object
Output parser
Postprocessor
Prompt
Query engine
Query transform
Retriever
Router
Synthesizer
Tool runner
Question Generators
Question Generators
Guidance
Llm question gen
Openai
Readers
Readers
Agent search
Airbyte cdk
Airbyte gong
Airbyte hubspot
Airbyte salesforce
Airbyte shopify
Airbyte stripe
Airbyte typeform
Airbyte zendesk support
Airtable
Apify
Arango db
Arxiv
Asana
Assemblyai
Astra db
Athena
Awadb
Azcognitive search
Azstorage blob
Azure devops
Bagel
Bilibili
Bitbucket
Boarddocs
Chatgpt plugin
Chroma
Clickhouse
Confluence
Couchbase
Couchdb
Dad jokes
Dashscope
Dashvector
Database
Deeplake
Discord
Docstring walker
Docugami
Earnings call transcript
Elasticsearch
Faiss
Feedly rss
Feishu docs
Feishu wiki
File
Firebase realtimedb
Firestore
Gcs
Genius
Github
Google
Gpt repo
Graphdb cypher
Graphql
Guru
Hatena blog
Hive
Hubspot
Huggingface fs
Hwp
Iceberg
Imdb review
Intercom
Jaguar
Jira
Joplin
Json
Kaltura esearch
Kibela
Lilac
Linear
Llama parse
Macrometa gdn
Make com
Mangadex
Mangoapps guides
Maps
Mbox
Memos
Metal
Microsoft onedrive
Microsoft outlook
Microsoft sharepoint
Milvus
Minio
Mondaydotcom
Mongodb
Myscale
Notion
Nougat ocr
Obsidian
Openalex
Openapi
Opendal
Opensearch
Pandas ai
Papers
Patentsview
Pathway
Pdb
Pdf marker
Pdf table
Pebblo
None
Preprocess
Psychic
Qdrant
Rayyan
Readme
Readwise
Reddit
Remote
Remote depth
S3
Sec filings
Semanticscholar
Simple directory reader
Singlestore
Slack
Smart pdf loader
Snowflake
Snscrape twitter
Spotify
Stackoverflow
Steamship
String iterable
Stripe docs
Structured data
Telegram
Toggl
Trello
Twitter
Txtai
Upstage
Weather
Weaviate
Web
Whatsapp
Wikipedia
Wordlift
Wordpress
Youtube metadata
Youtube transcript
Zendesk
Zep
Zulip
Response Synthesizers
Response Synthesizers
Accumulate
Compact accumulate
Compact and refine
Generation
Google
Refine
Simple summarize
Tree summarize
Retrievers
Retrievers
Auto merging
Bedrock
Bm25
Duckdb retriever
Keyword
Knowledge graph
Mongodb atlas bm25 retriever
Pathway
Query fusion
Recursive
Router
Sql
Summary
Transform
Tree
Vector
Videodb
You
Schema
Schema
Storage
Storage
Chat Store
Chat Store
Azure
Redis
Simple
Docstore
Docstore
Azure
Dynamodb
Elasticsearch
Firestore
Mongodb
Postgres
Redis
Simple
Graph Stores
Graph Stores
Falkordb
Kuzu
Nebula
Neo4j
Neptune
Simple
Tidb
Index Store
Index Store
Azure
Dynamodb
Elasticsearch
Firestore
Mongodb
Postgres
Redis
Simple
Kvstore
Kvstore
Azure
Dynamodb
Elasticsearch
Firestore
Mongodb
Postgres
Redis
S3
Simple
Storage
Storage
Storage context
Vector Store
Vector Store
Alibabacloud opensearch
Analyticdb
Astra db
Awadb
Awsdocdb
Azureaisearch
Azurecosmosmongo
Bagel
Baiduvectordb
Cassandra
Chatgpt plugin
Chroma
Clickhouse
Couchbase
Dashvector
Databricks
Deeplake
Docarray
Duckdb
Dynamodb
Elasticsearch
Epsilla
Faiss
Firestore
Google
Hologres
Jaguar
Kdbai
Lancedb
Lantern
Metal
Milvus
Mongodb
Myscale
Neo4jvector
Neptune
Opensearch
Pgvecto rs
Pinecone
Postgres
Qdrant
Redis
Relyt
Rocksetdb
Simple
Singlestoredb
Supabase
Tair
Tencentvectordb
Tidbvector
Timescalevector
Txtai
Typesense
Upstash
Vearch
Vertexaivectorsearch
Vespa
Weaviate
Wordlift
Zep
Tools
Tools
Arxiv
Azure code interpreter
Azure cv
Azure speech
Azure translate
Bing search
Brave search
Cassandra
Chatgpt plugin
Code interpreter
Cogniswitch
Database
Duckduckgo
Exa
Finance
Function
Google
Graphql
Ionic shopping
Jina
Load and search
Metaphor
Multion
Neo4j
Notion
Ondemand loader
Openai
Openapi
Passio nutrition ai
Playgrounds
Python file
Query engine
Query plan
Requests
Retriever
Salesforce
Shopify
Slack
Tavily research
Text to image
Tool spec
Vector db
Waii
Weather
Wikipedia
Wolfram alpha
Yahoo finance
Yelp
Zapier
Open-Source Community
Open-Source Community
Integrations
Full Stack Projects
Community FAQ
Community FAQ
Chat Engines
Documents and Nodes
Embeddings
Large Language Models
Query Engines
Vector Database
Contributing
Contributing
Code
Docs
Changelog
Presentations
Upgrading to v0.10.x
Deprecated Terms
LlamaCloud
LlamaCloud
LlamaParse
Table of contents
Base Types
BaseAgent
BaseAgentWorker
initialize_step
run_step
arun_step
stream_step
astream_step
finalize_task
set_callback_manager
as_agent
Task
TaskStep
get_next_step
link_step
TaskStepOutput
Runners
AgentRunner
create_task
delete_task
list_tasks
get_task
get_upcoming_steps
get_completed_steps
get_task_output
get_completed_tasks
run_step
arun_step
stream_step
astream_step
finalize_response
undo_step
ParallelAgentRunner
create_task
delete_task
get_completed_tasks
get_task_output
list_tasks
get_task
get_upcoming_steps
get_completed_steps
run_steps_in_queue
arun_steps_in_queue
run_step
arun_step
stream_step
astream_step
finalize_response
undo_step
Workers
CustomSimpleAgentWorker
from_tools
initialize_step
get_tools
run_step
arun_step
stream_step
astream_step
finalize_task
set_callback_manager
MultimodalReActAgentWorker
from_tools
initialize_step
get_tools
run_step
arun_step
stream_step
astream_step
finalize_task
set_callback_manager
QueryPipelineAgentWorker
agent_input_component
agent_components
preprocess
initialize_step
run_step
arun_step
stream_step
astream_step
finalize_task
set_callback_manager
Core Agent Classes#
Base Types#

Base agent types.

BaseAgent #

Bases: BaseChatEngine, BaseQueryEngine

Base Agent.

Source code in llama-index-core/llama_index/core/base/agent/types.py
BaseAgentWorker #

Bases: PromptMixin, DispatcherSpanMixin

Base agent worker.

Source code in llama-index-core/llama_index/core/base/agent/types.py
initialize_step abstractmethod #
initialize_step(task: Task, **kwargs: Any) -> TaskStep


Initialize step from task.

Source code in llama-index-core/llama_index/core/base/agent/types.py
run_step abstractmethod #
run_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step.

Source code in llama-index-core/llama_index/core/base/agent/types.py
arun_step abstractmethod async #
arun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step (async).

Source code in llama-index-core/llama_index/core/base/agent/types.py
stream_step abstractmethod #
stream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step (stream).

Source code in llama-index-core/llama_index/core/base/agent/types.py
astream_step abstractmethod async #
astream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step (async stream).

Source code in llama-index-core/llama_index/core/base/agent/types.py
finalize_task abstractmethod #
finalize_task(task: Task, **kwargs: Any) -> None


Finalize task, after all the steps are completed.

Source code in llama-index-core/llama_index/core/base/agent/types.py
set_callback_manager #
set_callback_manager(callback_manager: CallbackManager) -> None


Set callback manager.

Source code in llama-index-core/llama_index/core/base/agent/types.py
as_agent #
as_agent(**kwargs: Any) -> AgentRunner


Return as an agent runner.

Source code in llama-index-core/llama_index/core/base/agent/types.py
Task #

Bases: BaseModel

Agent Task.

Represents a "run" of an agent given a user input.

Source code in llama-index-core/llama_index/core/base/agent/types.py
TaskStep #

Bases: BaseModel

Agent task step.

Represents a single input step within the execution run ("Task") of an agent given a user input.

The output is returned as a TaskStepOutput.

Source code in llama-index-core/llama_index/core/base/agent/types.py
get_next_step #
get_next_step(step_id: str, input: Optional[str] = None, step_state: Optional[Dict[str, Any]] = None) -> TaskStep


Convenience function to get next step.

Preserve task_id, memory, step_state.

Source code in llama-index-core/llama_index/core/base/agent/types.py
link_step #
link_step(next_step: TaskStep) -> None


Link to next step.

Add link from this step to next, and from next step to current.

Source code in llama-index-core/llama_index/core/base/agent/types.py
TaskStepOutput #

Bases: BaseModel

Agent task step output.

Source code in llama-index-core/llama_index/core/base/agent/types.py
Runners#
AgentRunner #

Bases: BaseAgentRunner

Agent runner.

Top-level agent orchestrator that can create tasks, run each step in a task, or run a task e2e. Stores state and keeps track of tasks.

Parameters:

Name	Type	Description	Default
agent_worker	BaseAgentWorker	

step executor

	required
chat_history	Optional[List[ChatMessage]]	

chat history. Defaults to None.

	None
state	Optional[AgentState]	

agent state. Defaults to None.

	None
memory	Optional[BaseMemory]	

memory. Defaults to None.

	None
llm	Optional[LLM]	

LLM. Defaults to None.

	None
callback_manager	Optional[CallbackManager]	

callback manager. Defaults to None.

	None
init_task_state_kwargs	Optional[dict]	

init task state kwargs. Defaults to None.

	None
Source code in llama-index-core/llama_index/core/agent/runner/base.py
create_task #
create_task(input: str, **kwargs: Any) -> Task


Create task.

Source code in llama-index-core/llama_index/core/agent/runner/base.py
delete_task #
delete_task(task_id: str) -> None


Delete task.

NOTE: this will not delete any previous executions from memory.

Source code in llama-index-core/llama_index/core/agent/runner/base.py
list_tasks #
list_tasks(**kwargs: Any) -> List[Task]


List tasks.

Source code in llama-index-core/llama_index/core/agent/runner/base.py
get_task #
get_task(task_id: str, **kwargs: Any) -> Task


Get task.

Source code in llama-index-core/llama_index/core/agent/runner/base.py
get_upcoming_steps #
get_upcoming_steps(task_id: str, **kwargs: Any) -> List[TaskStep]


Get upcoming steps.

Source code in llama-index-core/llama_index/core/agent/runner/base.py
get_completed_steps #
get_completed_steps(task_id: str, **kwargs: Any) -> List[TaskStepOutput]


Get completed steps.

Source code in llama-index-core/llama_index/core/agent/runner/base.py
get_task_output #
get_task_output(task_id: str, **kwargs: Any) -> TaskStepOutput


Get task output.

Source code in llama-index-core/llama_index/core/agent/runner/base.py
get_completed_tasks #
get_completed_tasks(**kwargs: Any) -> List[Task]


Get completed tasks.

Source code in llama-index-core/llama_index/core/agent/runner/base.py
run_step #
run_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput


Run step.

Source code in llama-index-core/llama_index/core/agent/runner/base.py
arun_step async #
arun_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput


Run step (async).

Source code in llama-index-core/llama_index/core/agent/runner/base.py
stream_step #
stream_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput


Run step (stream).

Source code in llama-index-core/llama_index/core/agent/runner/base.py
astream_step async #
astream_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput


Run step (async stream).

Source code in llama-index-core/llama_index/core/agent/runner/base.py
finalize_response #
finalize_response(task_id: str, step_output: Optional[TaskStepOutput] = None) -> AGENT_CHAT_RESPONSE_TYPE


Finalize response.

Source code in llama-index-core/llama_index/core/agent/runner/base.py
undo_step #
undo_step(task_id: str) -> None


Undo previous step.

Source code in llama-index-core/llama_index/core/agent/runner/base.py
ParallelAgentRunner #

Bases: BaseAgentRunner

Parallel agent runner.

Executes steps in queue in parallel. Requires async support.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
create_task #
create_task(input: str, **kwargs: Any) -> Task


Create task.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
delete_task #
delete_task(task_id: str) -> None


Delete task.

NOTE: this will not delete any previous executions from memory.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
get_completed_tasks #
get_completed_tasks(**kwargs: Any) -> List[Task]


Get completed tasks.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
get_task_output #
get_task_output(task_id: str) -> TaskStepOutput


Get task output.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
list_tasks #
list_tasks(**kwargs: Any) -> List[Task]


List tasks.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
get_task #
get_task(task_id: str, **kwargs: Any) -> Task


Get task.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
get_upcoming_steps #
get_upcoming_steps(task_id: str, **kwargs: Any) -> List[TaskStep]


Get upcoming steps.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
get_completed_steps #
get_completed_steps(task_id: str, **kwargs: Any) -> List[TaskStepOutput]


Get completed steps.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
run_steps_in_queue #
run_steps_in_queue(task_id: str, mode: ChatResponseMode = ChatResponseMode.WAIT, **kwargs: Any) -> List[TaskStepOutput]


Execute steps in queue.

Run all steps in queue, clearing it out.

Assume that all steps can be run in parallel.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
arun_steps_in_queue async #
arun_steps_in_queue(task_id: str, mode: ChatResponseMode = ChatResponseMode.WAIT, **kwargs: Any) -> List[TaskStepOutput]


Execute all steps in queue.

All steps in queue are assumed to be ready.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
run_step #
run_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput


Run step.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
arun_step async #
arun_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput


Run step (async).

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
stream_step #
stream_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput


Run step (stream).

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
astream_step async #
astream_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput


Run step (async stream).

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
finalize_response #
finalize_response(task_id: str, step_output: Optional[TaskStepOutput] = None) -> AGENT_CHAT_RESPONSE_TYPE


Finalize response.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
undo_step #
undo_step(task_id: str) -> None


Undo previous step.

Source code in llama-index-core/llama_index/core/agent/runner/parallel.py
Workers#
CustomSimpleAgentWorker #

Bases: BaseModel, BaseAgentWorker

Custom simple agent worker.

This is "simple" in the sense that some of the scaffolding is setup already. Assumptions: - assumes that the agent has tools, llm, callback manager, and tool retriever - has a from_tools convenience function - assumes that the agent is sequential, and doesn't take in any additional intermediate inputs.

Parameters:

Name	Type	Description	Default
tools	Sequence[BaseTool]	

Tools to use for reasoning

	required
llm	LLM	

LLM to use

	required
callback_manager	CallbackManager	

Callback manager

	None
tool_retriever	Optional[ObjectRetriever[BaseTool]]	

Tool retriever

	None
verbose	bool	

Whether to print out reasoning steps

	False
Source code in llama-index-core/llama_index/core/agent/custom/simple.py
from_tools classmethod #
from_tools(tools: Optional[Sequence[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, llm: Optional[LLM] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> CustomSimpleAgentWorker


Convenience constructor method from set of of BaseTools (Optional).

Source code in llama-index-core/llama_index/core/agent/custom/simple.py
initialize_step #
initialize_step(task: Task, **kwargs: Any) -> TaskStep


Initialize step from task.

Source code in llama-index-core/llama_index/core/agent/custom/simple.py
get_tools #
get_tools(input: str) -> List[AsyncBaseTool]


Get tools.

Source code in llama-index-core/llama_index/core/agent/custom/simple.py
run_step #
run_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step.

Source code in llama-index-core/llama_index/core/agent/custom/simple.py
arun_step async #
arun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step (async).

Source code in llama-index-core/llama_index/core/agent/custom/simple.py
stream_step #
stream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step (stream).

Source code in llama-index-core/llama_index/core/agent/custom/simple.py
astream_step async #
astream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step (async stream).

Source code in llama-index-core/llama_index/core/agent/custom/simple.py
finalize_task #
finalize_task(task: Task, **kwargs: Any) -> None


Finalize task, after all the steps are completed.

Source code in llama-index-core/llama_index/core/agent/custom/simple.py
set_callback_manager #
set_callback_manager(callback_manager: CallbackManager) -> None


Set callback manager.

Source code in llama-index-core/llama_index/core/agent/custom/simple.py
MultimodalReActAgentWorker #

Bases: BaseAgentWorker

Multimodal ReAct Agent worker.

NOTE: This is a BETA feature.

Source code in llama-index-core/llama_index/core/agent/react_multimodal/step.py
from_tools classmethod #
from_tools(tools: Optional[Sequence[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, multi_modal_llm: Optional[MultiModalLLM] = None, max_iterations: int = 10, react_chat_formatter: Optional[ReActChatFormatter] = None, output_parser: Optional[ReActOutputParser] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> MultimodalReActAgentWorker


Convenience constructor method from set of of BaseTools (Optional).

NOTE: kwargs should have been exhausted by this point. In other words the various upstream components such as BaseSynthesizer (response synthesizer) or BaseRetriever should have picked up off their respective kwargs in their constructions.

Returns:

Type	Description
MultimodalReActAgentWorker	

ReActAgent

Source code in llama-index-core/llama_index/core/agent/react_multimodal/step.py
initialize_step #
initialize_step(task: Task, **kwargs: Any) -> TaskStep


Initialize step from task.

Source code in llama-index-core/llama_index/core/agent/react_multimodal/step.py
get_tools #
get_tools(input: str) -> List[AsyncBaseTool]


Get tools.

Source code in llama-index-core/llama_index/core/agent/react_multimodal/step.py
run_step #
run_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step.

Source code in llama-index-core/llama_index/core/agent/react_multimodal/step.py
arun_step async #
arun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step (async).

Source code in llama-index-core/llama_index/core/agent/react_multimodal/step.py
stream_step #
stream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step (stream).

Source code in llama-index-core/llama_index/core/agent/react_multimodal/step.py
astream_step async #
astream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step (async stream).

Source code in llama-index-core/llama_index/core/agent/react_multimodal/step.py
finalize_task #
finalize_task(task: Task, **kwargs: Any) -> None


Finalize task, after all the steps are completed.

Source code in llama-index-core/llama_index/core/agent/react_multimodal/step.py
set_callback_manager #
set_callback_manager(callback_manager: CallbackManager) -> None


Set callback manager.

Source code in llama-index-core/llama_index/core/agent/react_multimodal/step.py
QueryPipelineAgentWorker #

Bases: BaseModel, BaseAgentWorker

Query Pipeline agent worker.

NOTE: This is now deprecated. Use FnAgentWorker instead to build a stateful agent.

Barebones agent worker that takes in a query pipeline.

Default Workflow: The default workflow assumes that you compose a query pipeline with StatefulFnComponent objects. This allows you to store, update and retrieve state throughout the executions of the query pipeline by the agent.

The task and step state of the agent are stored in this state variable via a special key. Of course you can choose to store other variables in this state as well.

Deprecated Workflow: The deprecated workflow assumes that the first component in the query pipeline is an AgentInputComponent and last is AgentFnComponent.

Parameters:

Name	Type	Description	Default
pipeline	QueryPipeline	

Query pipeline

	required
Source code in llama-index-core/llama_index/core/agent/custom/pipeline_worker.py
agent_input_component property #
agent_input_component: AgentInputComponent


Get agent input component.

NOTE: This is deprecated and will be removed in the future.

agent_components property #
agent_components: List[AgentFnComponent]


Get agent output component.

preprocess #
preprocess(task: Task, step: TaskStep) -> None


Preprocessing flow.

This runs preprocessing to propagate the task and step as variables to relevant components in the query pipeline.

Contains deprecated flow of updating agent components. But also contains main flow of updating StatefulFnComponent components.

Source code in llama-index-core/llama_index/core/agent/custom/pipeline_worker.py
initialize_step #
initialize_step(task: Task, **kwargs: Any) -> TaskStep


Initialize step from task.

Source code in llama-index-core/llama_index/core/agent/custom/pipeline_worker.py
run_step #
run_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step.

Source code in llama-index-core/llama_index/core/agent/custom/pipeline_worker.py
arun_step async #
arun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step (async).

Source code in llama-index-core/llama_index/core/agent/custom/pipeline_worker.py
stream_step #
stream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step (stream).

Source code in llama-index-core/llama_index/core/agent/custom/pipeline_worker.py
astream_step async #
astream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput


Run step (async stream).

Source code in llama-index-core/llama_index/core/agent/custom/pipeline_worker.py
finalize_task #
finalize_task(task: Task, **kwargs: Any) -> None


Finalize task, after all the steps are completed.

Source code in llama-index-core/llama_index/core/agent/custom/pipeline_worker.py
set_callback_manager #
set_callback_manager(callback_manager: CallbackManager) -> None


Set callback manager.

Source code in llama-index-core/llama_index/core/agent/custom/pipeline_worker.py
 Back to top
Previous
Dashscope
Next
Introspective
