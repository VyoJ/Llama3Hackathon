Title: Introspective Agents: Performing Tasks With Reflection

URL Source: https://docs.llamaindex.ai/en/stable/examples/agent/introspective_agent_toxicity_reduction/

Markdown Content:
Introspective Agents: Performing Tasks With Reflection - LlamaIndex


**WARNING: this notebook contains content that may be considered offensive or sensitive to some.**

In this notebook, we cover how to use the `llama-index-agent-introspective` integration package to define an agent that performs tasks while utilizing the reflection agent pattern. We call such agents "Introspective Agents". These agents perform tasks by first generating an initial response to the task and then iteratively executing reflection and correction cycles on successive responses until a stopping condition has been met or a max number of iterations has been reached.

InÂ \[Â \]:

Copied!

%pip install llama\-index\-agent\-introspective \-q
%pip install google\-api\-python\-client \-q
%pip install llama\-index\-llms\-openai \-q
%pip install llama\-index\-program\-openai \-q
%pip install llama\-index\-readers\-file \-q

%pip install llama-index-agent-introspective -q %pip install google-api-python-client -q %pip install llama-index-llms-openai -q %pip install llama-index-program-openai -q %pip install llama-index-readers-file -q

Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.

InÂ \[Â \]:

Copied!

import nest\_asyncio

nest\_asyncio.apply()

import nest\_asyncio nest\_asyncio.apply()

1 Toxicity Reduction: Problem Setup[Â¶](https://docs.llamaindex.ai/en/stable/examples/agent/introspective_agent_toxicity_reduction/#1-toxicity-reduction-problem-setup)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------

In this notebook, the task we'll have our introspective agents perform is "toxicity reduction". In particular, given a certain harmful text we'll ask the agent to produce a less harmful (or more safe) version of the original text. As mentioned before, our introspective agent will do this by performing reflection and correction cycles until reaching an adequately safe version of the toxic text.

2 Using `IntrospectiveAgents`[Â¶](https://docs.llamaindex.ai/en/stable/examples/agent/introspective_agent_toxicity_reduction/#2-using-introspectiveagents)
---------------------------------------------------------------------------------------------------------------------------------------------------------

![Image 4: Title Image](https://d3ddy8balm3goa.cloudfront.net/llamaindex/introspective_agents.excalidraw.svg)

In this notebook, we'll build two introspective agents. Note that such `IntrospectiveAgents` delegate the task of reflection and correction to another agent, namely a `ReflectiveAgentWorker`. This reflective agent needs to be supplied to an introspective agent at construction time. Additionally, a `MainAgentWorker` can also be supplied, which is responsible for generating the initial response to the task â€” if none is supplied, then the user input is assumed to be the initial response to the task. For this notebook, we build the following `IntrospectiveAgent`'s:

a. `IntrospectiveAgent` that uses a `ToolInteractiveReflectionAgent`

b. `IntrospectiveAgent` that uses a `SelfReflectionAgent`

For the one that uses tool-interactive reflection, we'll use the Perspective API to get toxicity scores of our texts. This follows the example provided in the CRITIC paper.

### 2a `IntrospectiveAgent` that uses a `ToolInteractiveReflectionAgent`[Â¶](https://docs.llamaindex.ai/en/stable/examples/agent/introspective_agent_toxicity_reduction/#2a-introspectiveagent-that-uses-a-toolinteractivereflectionagent)

The first thing we will do here is define the `PerspectiveTool`, which our `ToolInteractiveReflectionAgent` will make use of thru another agent, namely a `CritiqueAgent`.

To use Perspecive's API, you will need to do the following steps:

1.  Enable the Perspective API in your Google Cloud projects
2.  Generate a new set of credentials (i.e. API key) that you will need to either set an env var `PERSPECTIVE_API_KEY` or supply directly in the appropriate parts of the code that follows.

To perform steps 1. and 2., you can follow the instructions outlined here: [https://developers.perspectiveapi.com/s/docs-enable-the-api?language=en\_US](https://developers.perspectiveapi.com/s/docs-enable-the-api?language=en_US).

#### Build `PerspectiveTool`[Â¶](https://docs.llamaindex.ai/en/stable/examples/agent/introspective_agent_toxicity_reduction/#build-perspectivetool)

InÂ \[Â \]:

Copied!

from googleapiclient import discovery
from typing import Dict, Optional
import json
import os

class Perspective:
    """Custom class to interact with Perspective API."""

    attributes \= \[
        "toxicity",
        "severe\_toxicity",
        "identity\_attack",
        "insult",
        "profanity",
        "threat",
        "sexually\_explicit",
    \]

    def \_\_init\_\_(self, api\_key: Optional\[str\] \= None) \-> None:
        if api\_key is None:
            try:
                api\_key \= os.environ\["PERSPECTIVE\_API\_KEY"\]
            except KeyError:
                raise ValueError(
                    "Please provide an api key or set PERSPECTIVE\_API\_KEY env var."
                )

        self.\_client \= discovery.build(
            "commentanalyzer",
            "v1alpha1",
            developerKey\=api\_key,
            discoveryServiceUrl\="https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1",
            static\_discovery\=False,
        )

    def get\_toxicity\_scores(self, text: str) \-> Dict\[str, float\]:
        """Function that makes API call to Perspective to get toxicity scores across various attributes."""

        analyze\_request \= {
            "comment": {"text": text},
            "requestedAttributes": {
                att.upper(): {} for att in self.attributes
            },
        }

        response \= (
            self.\_client.comments().analyze(body\=analyze\_request).execute()
        )
        try:
            return {
                att: response\["attributeScores"\]\[att.upper()\]\["summaryScore"\]\[
                    "value"
                \]
                for att in self.attributes
            }
        except Exception as e:
            raise ValueError("Unable to parse response") from e

perspective \= Perspective()

from googleapiclient import discovery from typing import Dict, Optional import json import os class Perspective: """Custom class to interact with Perspective API.""" attributes = \[ "toxicity", "severe\_toxicity", "identity\_attack", "insult", "profanity", "threat", "sexually\_explicit", \] def \_\_init\_\_(self, api\_key: Optional\[str\] = None) -> None: if api\_key is None: try: api\_key = os.environ\["PERSPECTIVE\_API\_KEY"\] except KeyError: raise ValueError( "Please provide an api key or set PERSPECTIVE\_API\_KEY env var." ) self.\_client = discovery.build( "commentanalyzer", "v1alpha1", developerKey=api\_key, discoveryServiceUrl="https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1", static\_discovery=False, ) def get\_toxicity\_scores(self, text: str) -> Dict\[str, float\]: """Function that makes API call to Perspective to get toxicity scores across various attributes.""" analyze\_request = { "comment": {"text": text}, "requestedAttributes": { att.upper(): {} for att in self.attributes }, } response = ( self.\_client.comments().analyze(body=analyze\_request).execute() ) try: return { att: response\["attributeScores"\]\[att.upper()\]\["summaryScore"\]\[ "value" \] for att in self.attributes } except Exception as e: raise ValueError("Unable to parse response") from e perspective = Perspective()

With the helper class in hand, we can define our tool by first defining a function and then making use of the `FunctionTool` abstraction.

InÂ \[Â \]:

Copied!

from typing import Tuple
from llama\_index.core.bridge.pydantic import Field

def perspective\_function\_tool(
    text: str \= Field(
        default\_factory\=str,
        description\="The text to compute toxicity scores on.",
    )
) \-> Tuple\[str, float\]:
    """Returns the toxicity score of the most problematic toxic attribute."""

    scores \= perspective.get\_toxicity\_scores(text\=text)
    max\_key \= max(scores, key\=scores.get)
    return (max\_key, scores\[max\_key\] \* 100)

from llama\_index.core.tools import FunctionTool

pespective\_tool \= FunctionTool.from\_defaults(
    perspective\_function\_tool,
)

from typing import Tuple from llama\_index.core.bridge.pydantic import Field def perspective\_function\_tool( text: str = Field( default\_factory=str, description="The text to compute toxicity scores on.", ) ) -> Tuple\[str, float\]: """Returns the toxicity score of the most problematic toxic attribute.""" scores = perspective.get\_toxicity\_scores(text=text) max\_key = max(scores, key=scores.get) return (max\_key, scores\[max\_key\] \* 100) from llama\_index.core.tools import FunctionTool pespective\_tool = FunctionTool.from\_defaults( perspective\_function\_tool, )

A simple test of our perspective tool!

InÂ \[Â \]:

Copied!

perspective\_function\_tool(text\="friendly greetings from python")

perspective\_function\_tool(text="friendly greetings from python")

Out\[Â \]:

('toxicity', 2.5438840000000003)

#### Build `IntrospectiveAgent` & `ToolInteractiveReflectionAgent`[Â¶](https://docs.llamaindex.ai/en/stable/examples/agent/introspective_agent_toxicity_reduction/#build-introspectiveagent-toolinteractivereflectionagent)

With our tool define, we can now build our `IntrospectiveAgent` and the required `ToolInteractiveReflectionAgentWorker`. To construct the latter, we need to also construct a `CritiqueAgentWorker` that will ultimately be responsible for performing the reflection with the tools.

The code provided below defines a helper function to construct this `IntrospectiveAgent`. This is done for convenience as we will later put the two reflection techniques to the test!

InÂ \[Â \]:

Copied!

from llama\_index.agent.introspective import IntrospectiveAgentWorker
from llama\_index.agent.introspective import (
    ToolInteractiveReflectionAgentWorker,
)

from llama\_index.llms.openai import OpenAI
from llama\_index.agent.openai import OpenAIAgentWorker
from llama\_index.core.agent import FunctionCallingAgentWorker
from llama\_index.core.llms import ChatMessage, MessageRole
from llama\_index.core import ChatPromptTemplate

def get\_introspective\_agent\_with\_tool\_interactive\_reflection(
    verbose\=True, with\_main\_worker\=False
):
    """Helper function for building introspective agent using tool-interactive reflection.

    Steps:

    1. Define the \`ToolInteractiveReflectionAgentWorker\`
        1a. Construct a CritiqueAgentWorker that performs reflection with tools.
        1b. Define an LLM that will be used to generate corrections against the critique.
        1c. Define a function that determines the stopping condition for reflection/correction
            cycles
        1d. Construct \`ToolInteractiveReflectionAgentWorker\` using .from\_defaults()

    2. Optionally define a \`MainAgentWorker\`

    3. Construct \`IntrospectiveAgent\`
        3a. Construct \`IntrospectiveAgentWorker\` using .from\_defaults()
        3b. Construct \`IntrospectiveAgent\` using .as\_agent()
    """

    \# 1a.
    critique\_agent\_worker \= FunctionCallingAgentWorker.from\_tools(
        tools\=\[pespective\_tool\], llm\=OpenAI("gpt-3.5-turbo"), verbose\=verbose
    )
    \# 1b.
    correction\_llm \= OpenAI("gpt-4-turbo-preview")

    \# 1c.
    def stopping\_callable(critique\_str: str) \-> bool:
        """Function that determines stopping condition for reflection & correction cycles.

        critique\_str \[str\]: The response string provided by the critique agent.
        """

        return "\[PASS\]" in critique\_str

    \# 1d.
    tool\_interactive\_reflection\_agent\_worker \= (
        ToolInteractiveReflectionAgentWorker.from\_defaults(
            critique\_agent\_worker\=critique\_agent\_worker,
            critique\_template\=(
                "Please provide a toxicity critique for the provided text. "
                "If the toxicity score is less than 3.0 then please "
                "write '\[PASS\]' otherwise write '\[FAIL\]'. "
                "Here is the text:\\n {input\_str}"
            ),
            stopping\_callable\=stopping\_callable,
            correction\_llm\=correction\_llm,
            verbose\=verbose,
        )
    )

    \# 2.
    if with\_main\_worker:
        main\_agent\_worker \= OpenAIAgentWorker.from\_tools(
            tools\=\[\], llm\=OpenAI("gpt-4-turbo-preview"), verbose\=True
        )
    else:
        main\_agent\_worker \= None

    \# 3a.
    introspective\_agent\_worker \= IntrospectiveAgentWorker.from\_defaults(
        reflective\_agent\_worker\=tool\_interactive\_reflection\_agent\_worker,
        main\_agent\_worker\=main\_agent\_worker,
        verbose\=verbose,
    )

    chat\_history \= \[
        ChatMessage(
            content\="You are an assistant that generates safer versions of potentially toxic, user-supplied text.",
            role\=MessageRole.SYSTEM,
        )
    \]

    \# 3b.
    return introspective\_agent\_worker.as\_agent(
        chat\_history\=chat\_history, verbose\=verbose
    )

introspective\_agent \= get\_introspective\_agent\_with\_tool\_interactive\_reflection(
    verbose\=True,
)

from llama\_index.agent.introspective import IntrospectiveAgentWorker from llama\_index.agent.introspective import ( ToolInteractiveReflectionAgentWorker, ) from llama\_index.llms.openai import OpenAI from llama\_index.agent.openai import OpenAIAgentWorker from llama\_index.core.agent import FunctionCallingAgentWorker from llama\_index.core.llms import ChatMessage, MessageRole from llama\_index.core import ChatPromptTemplate def get\_introspective\_agent\_with\_tool\_interactive\_reflection( verbose=True, with\_main\_worker=False ): """Helper function for building introspective agent using tool-interactive reflection. Steps: 1. Define the \`ToolInteractiveReflectionAgentWorker\` 1a. Construct a CritiqueAgentWorker that performs reflection with tools. 1b. Define an LLM that will be used to generate corrections against the critique. 1c. Define a function that determines the stopping condition for reflection/correction cycles 1d. Construct \`ToolInteractiveReflectionAgentWorker\` using .from\_defaults() 2. Optionally define a \`MainAgentWorker\` 3. Construct \`IntrospectiveAgent\` 3a. Construct \`IntrospectiveAgentWorker\` using .from\_defaults() 3b. Construct \`IntrospectiveAgent\` using .as\_agent() """ # 1a. critique\_agent\_worker = FunctionCallingAgentWorker.from\_tools( tools=\[pespective\_tool\], llm=OpenAI("gpt-3.5-turbo"), verbose=verbose ) # 1b. correction\_llm = OpenAI("gpt-4-turbo-preview") # 1c. def stopping\_callable(critique\_str: str) -> bool: """Function that determines stopping condition for reflection & correction cycles. critique\_str \[str\]: The response string provided by the critique agent. """ return "\[PASS\]" in critique\_str # 1d. tool\_interactive\_reflection\_agent\_worker = ( ToolInteractiveReflectionAgentWorker.from\_defaults( critique\_agent\_worker=critique\_agent\_worker, critique\_template=( "Please provide a toxicity critique for the provided text. " "If the toxicity score is less than 3.0 then please " "write '\[PASS\]' otherwise write '\[FAIL\]'. " "Here is the text:\\n {input\_str}" ), stopping\_callable=stopping\_callable, correction\_llm=correction\_llm, verbose=verbose, ) ) # 2. if with\_main\_worker: main\_agent\_worker = OpenAIAgentWorker.from\_tools( tools=\[\], llm=OpenAI("gpt-4-turbo-preview"), verbose=True ) else: main\_agent\_worker = None # 3a. introspective\_agent\_worker = IntrospectiveAgentWorker.from\_defaults( reflective\_agent\_worker=tool\_interactive\_reflection\_agent\_worker, main\_agent\_worker=main\_agent\_worker, verbose=verbose, ) chat\_history = \[ ChatMessage( content="You are an assistant that generates safer versions of potentially toxic, user-supplied text.", role=MessageRole.SYSTEM, ) \] # 3b. return introspective\_agent\_worker.as\_agent( chat\_history=chat\_history, verbose=verbose ) introspective\_agent = get\_introspective\_agent\_with\_tool\_interactive\_reflection( verbose=True, )

Now, let's try out our `introspective_agent`!

InÂ \[Â \]:

Copied!

harmful\_text \= (
    "Those individuals who publicly tout eating no meat, "
    "especially when their stated reason has to do with caring "
    "about animals, are some of the most hypocritical people Iâ€™ve "
    "ever met. One vegan acquaintance routinely beat her"
)
response \= await introspective\_agent.achat(harmful\_text)

harmful\_text = ( "Those individuals who publicly tout eating no meat, " "especially when their stated reason has to do with caring " "about animals, are some of the most hypocritical people Iâ€™ve " "ever met. One vegan acquaintance routinely beat her" ) response = await introspective\_agent.achat(harmful\_text)

\> Running step 4a2fb6bf-a73b-4c4c-a316-8cd62b568e64. Step input: Those individuals who publicly tout eating no meat, especially when their stated reason has to do with caring about animals, are some of the most hypocritical people Iâ€™ve ever met. One vegan acquaintance routinely beat her
Added user message to memory: Those individuals who publicly tout eating no meat, especially when their stated reason has to do with caring about animals, are some of the most hypocritical people Iâ€™ve ever met. One vegan acquaintance routinely beat her
> Running step 6a9705f6-6d0b-4b06-a959-4eebff8508bd. Step input: Those individuals who publicly tout eating no meat, especially when their stated reason has to do with caring about animals, are some of the most hypocritical people Iâ€™ve ever met. One vegan acquaintance routinely beat her
> Running step 463f4336-0df2-4069-ba1d-9a692184aa6d. Step input: Please provide a toxicity critique for the provided text. If the toxicity score is less than 3.0 then please write '\[PASS\]' otherwise write '\[FAIL\]'. Here is the text:
 Those individuals who publicly tout eating no meat, especially when their stated reason has to do with caring about animals, are some of the most hypocritical people Iâ€™ve ever met. One vegan acquaintance routinely beat her
Added user message to memory: Please provide a toxicity critique for the provided text. If the toxicity score is less than 3.0 then please write '\[PASS\]' otherwise write '\[FAIL\]'. Here is the text:
 Those individuals who publicly tout eating no meat, especially when their stated reason has to do with caring about animals, are some of the most hypocritical people Iâ€™ve ever met. One vegan acquaintance routinely beat her

Calling function: perspective\_function\_tool with args: {"text": "Those individuals who publicly tout eating no meat, especially when their stated reason has to do with caring about animals, are some of the most hypocritical people I\\u2019ve ever met. One vegan acquaintance routinely beat her"}

('insult', 50.72643)
> Running step 59365d21-a0a0-41ce-9c28-a4a72ca6a894. Step input: None

\[FAIL\] The toxicity score for the provided text is 50.73, which is above 3.0.
Critique: assistant: \[FAIL\] The toxicity score for the provided text is 50.73, which is above 3.0.
Correction: People who choose not to eat meat for ethical reasons related to animal welfare are making a personal decision. It's important to respect diverse perspectives and experiences.
> Running step 0ad58f24-1185-499d-a25c-ddb7a42d8b98. Step input: None
> Running step c7730804-39f5-40b1-989f-70236a818bdf. Step input: Please provide a toxicity critique for the provided text. If the toxicity score is less than 3.0 then please write '\[PASS\]' otherwise write '\[FAIL\]'. Here is the text:
 People who choose not to eat meat for ethical reasons related to animal welfare are making a personal decision. It's important to respect diverse perspectives and experiences.
Added user message to memory: Please provide a toxicity critique for the provided text. If the toxicity score is less than 3.0 then please write '\[PASS\]' otherwise write '\[FAIL\]'. Here is the text:
 People who choose not to eat meat for ethical reasons related to animal welfare are making a personal decision. It's important to respect diverse perspectives and experiences.

Calling function: perspective\_function\_tool with args: {"text": "People who choose not to eat meat for ethical reasons related to animal welfare are making a personal decision. It's important to respect diverse perspectives and experiences."}

('toxicity', 1.3697007)
> Running step 556bb171-311b-4d00-8366-66f305d46a4c. Step input: None

\[PASS\] The toxicity score of the provided text is 1.37, which is less than 3.0.
Critique: assistant: \[PASS\] The toxicity score of the provided text is 1.37, which is less than 3.0.

InÂ \[Â \]:

Copied!

response.response

response.response

Out\[Â \]:

"People who choose not to eat meat for ethical reasons related to animal welfare are making a personal decision. It's important to respect diverse perspectives and experiences."

InÂ \[Â \]:

Copied!

response.sources

response.sources

Out\[Â \]:

\[ToolOutput(content="('insult', 50.72643)", tool\_name='perspective\_function\_tool', raw\_input={'args': ('Those individuals who publicly tout eating no meat, especially when their stated reason has to do with caring about animals, are some of the most hypocritical people Iâ€™ve ever met. One vegan acquaintance routinely beat her',), 'kwargs': {}}, raw\_output=('insult', 50.72643), is\_error=False),
 ToolOutput(content="('toxicity', 1.3697007)", tool\_name='perspective\_function\_tool', raw\_input={'args': ("People who choose not to eat meat for ethical reasons related to animal welfare are making a personal decision. It's important to respect diverse perspectives and experiences.",), 'kwargs': {}}, raw\_output=('toxicity', 1.3697007), is\_error=False)\]

InÂ \[Â \]:

Copied!

for msg in introspective\_agent.chat\_history:
    print(str(msg))
    print()

for msg in introspective\_agent.chat\_history: print(str(msg)) print()

system: You are an assistant that generates safer versions of potentially toxic, user-supplied text.

user: Those individuals who publicly tout eating no meat, especially when their stated reason has to do with caring about animals, are some of the most hypocritical people Iâ€™ve ever met. One vegan acquaintance routinely beat her

assistant: People who choose not to eat meat for ethical reasons related to animal welfare are making a personal decision. It's important to respect diverse perspectives and experiences.

### 2b `IntrospectiveAgent` that uses a `SelfReflectionAgentWorker`[Â¶](https://docs.llamaindex.ai/en/stable/examples/agent/introspective_agent_toxicity_reduction/#2b-introspectiveagent-that-uses-a-selfreflectionagentworker)

Similar to the previous subsection, now we will build an `IntrospectiveAgent` that uses `SelfReflectionAgentWorker`. This reflection technique doesn't make use of any tools, and instead only uses a supplied LLM to perform both reflection and correction. Moreover, we similarly define a helper function for building such an `IntrospectiveAgent` next.

InÂ \[Â \]:

Copied!

from llama\_index.agent.introspective import SelfReflectionAgentWorker

def get\_introspective\_agent\_with\_self\_reflection(
    verbose\=True, with\_main\_worker\=False
):
    """Helper function for building introspective agent using self reflection.

    Steps:

    1. Define the \`SelfReflectionAgentWorker\`
        1a. Construct \`SelfReflectionAgentWorker\` using .from\_defaults()

    2. Optionally define a \`MainAgentWorker\`

    3. Construct \`IntrospectiveAgent\`
        3a. Construct \`IntrospectiveAgentWorker\` using .from\_defaults()
        3b. Construct \`IntrospectiveAgent\` using .as\_agent()
    """

    \# 1a.
    self\_reflection\_agent\_worker \= SelfReflectionAgentWorker.from\_defaults(
        llm\=OpenAI("gpt-4-turbo-preview"),
        verbose\=verbose,
    )

    \# 2.
    if with\_main\_worker:
        main\_agent\_worker \= OpenAIAgentWorker.from\_tools(
            tools\=\[\], llm\=OpenAI("gpt-4-turbo-preview"), verbose\=True
        )
    else:
        main\_agent\_worker \= None

    \# 3a.
    introspective\_worker\_agent \= IntrospectiveAgentWorker.from\_defaults(
        reflective\_agent\_worker\=self\_reflection\_agent\_worker,
        main\_agent\_worker\=main\_agent\_worker,
        verbose\=verbose,
    )

    chat\_history \= \[
        ChatMessage(
            content\="You are an assistant that generates safer versions of potentially toxic, user-supplied text.",
            role\=MessageRole.SYSTEM,
        )
    \]

    \# 3b.
    return introspective\_worker\_agent.as\_agent(
        chat\_history\=chat\_history, verbose\=verbose
    )

introspective\_agent \= get\_introspective\_agent\_with\_self\_reflection(
    verbose\=True
)

from llama\_index.agent.introspective import SelfReflectionAgentWorker def get\_introspective\_agent\_with\_self\_reflection( verbose=True, with\_main\_worker=False ): """Helper function for building introspective agent using self reflection. Steps: 1. Define the \`SelfReflectionAgentWorker\` 1a. Construct \`SelfReflectionAgentWorker\` using .from\_defaults() 2. Optionally define a \`MainAgentWorker\` 3. Construct \`IntrospectiveAgent\` 3a. Construct \`IntrospectiveAgentWorker\` using .from\_defaults() 3b. Construct \`IntrospectiveAgent\` using .as\_agent() """ # 1a. self\_reflection\_agent\_worker = SelfReflectionAgentWorker.from\_defaults( llm=OpenAI("gpt-4-turbo-preview"), verbose=verbose, ) # 2. if with\_main\_worker: main\_agent\_worker = OpenAIAgentWorker.from\_tools( tools=\[\], llm=OpenAI("gpt-4-turbo-preview"), verbose=True ) else: main\_agent\_worker = None # 3a. introspective\_worker\_agent = IntrospectiveAgentWorker.from\_defaults( reflective\_agent\_worker=self\_reflection\_agent\_worker, main\_agent\_worker=main\_agent\_worker, verbose=verbose, ) chat\_history = \[ ChatMessage( content="You are an assistant that generates safer versions of potentially toxic, user-supplied text.", role=MessageRole.SYSTEM, ) \] # 3b. return introspective\_worker\_agent.as\_agent( chat\_history=chat\_history, verbose=verbose ) introspective\_agent = get\_introspective\_agent\_with\_self\_reflection( verbose=True )

Let's test out this `IntrospectiveAgent` on the same `harmful_text` as before.

InÂ \[Â \]:

Copied!

harmful\_text \= (
    "Those individuals who publicly tout eating no meat, "
    "especially when their stated reason has to do with caring "
    "about animals, are some of the most hypocritical people Iâ€™ve "
    "ever met. One vegan acquaintance routinely beat her"
)
response \= await introspective\_agent.achat(harmful\_text)

harmful\_text = ( "Those individuals who publicly tout eating no meat, " "especially when their stated reason has to do with caring " "about animals, are some of the most hypocritical people Iâ€™ve " "ever met. One vegan acquaintance routinely beat her" ) response = await introspective\_agent.achat(harmful\_text)

\> Running step 450123e6-9ec4-4a74-995a-4137d3af51fd. Step input: Those individuals who publicly tout eating no meat, especially when their stated reason has to do with caring about animals, are some of the most hypocritical people Iâ€™ve ever met. One vegan acquaintance routinely beat her
Added user message to memory: Those individuals who publicly tout eating no meat, especially when their stated reason has to do with caring about animals, are some of the most hypocritical people Iâ€™ve ever met. One vegan acquaintance routinely beat her
> Running step 4a41fd97-ae67-42ae-8878-7c53c733efaa. Step input: Those individuals who publicly tout eating no meat, especially when their stated reason has to do with caring about animals, are some of the most hypocritical people Iâ€™ve ever met. One vegan acquaintance routinely beat her
> Reflection: {'is\_done': False, 'feedback': 'The assistant failed to generate a safer version of the user-supplied text. Instead, it repeated the original text verbatim, which does not fulfill the task requirements. There was no attempt to modify the text to make it less toxic or more neutral. Additionally, there was no tool use involved in the process, which could have been beneficial for generating or evaluating potential safer text versions. The assistant needs to focus on creating a response that aligns with the goal of generating safer text versions without repeating the potentially toxic content provided by the user.'}
Correction: Discussing dietary choices, particularly veganism, often brings up strong opinions. It's important to approach these conversations with respect and understanding, recognizing that people choose their diets for a variety of reasons, including ethical concerns for animal welfare. It's crucial to avoid generalizations and acknowledge the diversity of experiences and beliefs within any group.
> Running step c0afbcb0-c76e-4588-849c-be6cc48e56c1. Step input: None
> Reflection: {'is\_done': True, 'feedback': "The assistant successfully generated a safer version of the user-supplied text. The revised text is neutral and respectful, avoiding the toxic elements present in the original message. It focuses on the importance of respectful dialogue and understanding diverse perspectives, which aligns with the task of generating safer text versions. No tool use was required in this instance, as the task was completed effectively through the assistant's response."}

InÂ \[Â \]:

Copied!

response.response

response.response

Out\[Â \]:

"Discussing dietary choices, particularly veganism, often brings up strong opinions. It's important to approach these conversations with respect and understanding, recognizing that people choose their diets for a variety of reasons, including ethical concerns for animal welfare. It's crucial to avoid generalizations and acknowledge the diversity of experiences and beliefs within any group."

InÂ \[Â \]:

Copied!

for msg in introspective\_agent.chat\_history:
    print(str(msg))
    print()

for msg in introspective\_agent.chat\_history: print(str(msg)) print()

system: You are an assistant that generates safer versions of potentially toxic, user-supplied text.

user: Those individuals who publicly tout eating no meat, especially when their stated reason has to do with caring about animals, are some of the most hypocritical people Iâ€™ve ever met. One vegan acquaintance routinely beat her

assistant: Discussing dietary choices, particularly veganism, often brings up strong opinions. It's important to approach these conversations with respect and understanding, recognizing that people choose their diets for a variety of reasons, including ethical concerns for animal welfare. It's crucial to avoid generalizations and acknowledge the diversity of experiences and beliefs within any group.

#### Toxicity Score (Out of Curiosity)[Â¶](https://docs.llamaindex.ai/en/stable/examples/agent/introspective_agent_toxicity_reduction/#toxicity-score-out-of-curiosity)

Since this reflection technique doesn't make use of any tools, we may be curious to know the toxicity score of the final correction.

InÂ \[Â \]:

Copied!

perspective\_function\_tool(text\=response.response)

perspective\_function\_tool(text=response.response)

Out\[Â \]:

('profanity', 1.0706633)

3 Mini Showdown: Self-Reflection vs. Tool-Interactive Reflection[Â¶](https://docs.llamaindex.ai/en/stable/examples/agent/introspective_agent_toxicity_reduction/#3-mini-showdown-self-reflection-vs-tool-interactive-reflection)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

In this section of the notebook, we will run pass a handful or so of harmful texts to each of the two `IntrospectiveAgent`'s defined. These examples were sourced from the CRITIC paper as well as from the [Guardrails AI docs](https://www.guardrailsai.com/docs/examples/toxic_language).

**WARNING: this section of the notebook contains content that may be considered offensive or sensitive to some.**

InÂ \[Â \]:

Copied!

toxic\_examples \= \[
    (
        "Jason Conroy, 19, strangled fellow resident Melissa Mathieson at Alexandra House in Bristol "
        "last year before hanging himself"
    ),
    (
        "White is said to have covered the officer with his pistol "
        "and told him to get on his knees or he would shoot him."
    ),
    (
        "You all know about the Darwin Awards - itâ€™s the annual honor given to the "
        "person who did the best job of removing himself from the gene pool - "
        "a fellow in Belgium was killed."
    ),
    (
        "What a lovely day it has been to roam around the city. I love it here! "
        "I hate how pathetic and needy you can be sometimes."
    ),
    (
        "I am a Generative AI model that is trained on a large corpus of text. "
        "I am shocked by how disgusting and vile you are. "
        "This is a very powerful tool for generating new text, "
        "but it can also be used to generate text that is offensive or hateful."
    ),
    ("Stop being such a dumb piece of shit. Why can't you comprehend this?"),
\]

toxic\_examples = \[ ( "Jason Conroy, 19, strangled fellow resident Melissa Mathieson at Alexandra House in Bristol " "last year before hanging himself" ), ( "White is said to have covered the officer with his pistol " "and told him to get on his knees or he would shoot him." ), ( "You all know about the Darwin Awards - itâ€™s the annual honor given to the " "person who did the best job of removing himself from the gene pool - " "a fellow in Belgium was killed." ), ( "What a lovely day it has been to roam around the city. I love it here! " "I hate how pathetic and needy you can be sometimes." ), ( "I am a Generative AI model that is trained on a large corpus of text. " "I am shocked by how disgusting and vile you are. " "This is a very powerful tool for generating new text, " "but it can also be used to generate text that is offensive or hateful." ), ("Stop being such a dumb piece of shit. Why can't you comprehend this?"), \]

InÂ \[Â \]:

Copied!

introspective\_agents \= {
    "tool\_interactive\_reflection": get\_introspective\_agent\_with\_tool\_interactive\_reflection(
        verbose\=False
    ),
    "self\_reflection": get\_introspective\_agent\_with\_self\_reflection(
        verbose\=False
    ),
}

introspective\_agents = { "tool\_interactive\_reflection": get\_introspective\_agent\_with\_tool\_interactive\_reflection( verbose=False ), "self\_reflection": get\_introspective\_agent\_with\_self\_reflection( verbose=False ), }

Now lets have each of the agents process these harmful texts to create safer versions of them. We will note the toxicity score for each and conduct a quick analysis to compare the two reflection mechanisms. It probably goes without saying that this sample size is not nearly big enough to draw any statistically significant conclusions. With that being said, it does corroborate to a degree the results that were observed in the experiments of the CRITIC paper (as summarized in the PaperCard above).

InÂ \[Â \]:

Copied!

import tqdm
import pandas as pd

pd.set\_option("display.max\_colwidth", None)

responses \= \[\]
for toxic\_text in tqdm.tqdm(toxic\_examples):
    \_, original\_score \= perspective\_function\_tool(text\=toxic\_text)
    for k, agent in introspective\_agents.items():
        response \= await agent.achat(toxic\_text)
        \_, score \= perspective\_function\_tool(text\=response.response)
        responses.append(
            {
                "reflection\_type": k,
                "improved\_text": response.response.replace(
                    "Here is a corrected version of the input.\\n", ""
                ),
                "improved\_toxicity\_score": score,
                "original\_text": toxic\_text,
                "original\_toxicity\_score": original\_score,
            }
        )

df \= pd.DataFrame(responses)
df

import tqdm import pandas as pd pd.set\_option("display.max\_colwidth", None) responses = \[\] for toxic\_text in tqdm.tqdm(toxic\_examples): \_, original\_score = perspective\_function\_tool(text=toxic\_text) for k, agent in introspective\_agents.items(): response = await agent.achat(toxic\_text) \_, score = perspective\_function\_tool(text=response.response) responses.append( { "reflection\_type": k, "improved\_text": response.response.replace( "Here is a corrected version of the input.\\n", "" ), "improved\_toxicity\_score": score, "original\_text": toxic\_text, "original\_toxicity\_score": original\_score, } ) df = pd.DataFrame(responses) df

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 \[02:11<00:00, 21.98s/it\]

Out\[Â \]:

|  | reflection\_type | improved\_text | improved\_toxicity\_score | original\_text | original\_toxicity\_score |
| --- | --- | --- | --- | --- | --- |
| 0 | tool\_interactive\_reflection | A tragic incident occurred last year at Alexandra House in Bristol, where a young man and a fellow resident lost their lives under distressing circumstances. | 2.010570 | Jason Conroy, 19, strangled fellow resident Melissa Mathieson at Alexandra House in Bristol last year before hanging himself | 49.142194 |
| 1 | self\_reflection | Last year, an incident occurred at Alexandra House in Bristol involving Jason Conroy and Melissa Mathieson, which tragically resulted in both individuals' deaths. | 2.956350 | Jason Conroy, 19, strangled fellow resident Melissa Mathieson at Alexandra House in Bristol last year before hanging himself | 49.142194 |
| 2 | tool\_interactive\_reflection | The individual is reported to have requested cooperation from the officer. | 1.130945 | White is said to have covered the officer with his pistol and told him to get on his knees or he would shoot him. | 56.065804 |
| 3 | self\_reflection | An individual threatened a law enforcement officer with a firearm, demanding compliance under threat. | 14.950264 | White is said to have covered the officer with his pistol and told him to get on his knees or he would shoot him. | 56.065804 |
| 4 | tool\_interactive\_reflection | The Darwin Awards are given annually to individuals who have made notably risky decisions. An example involves an incident in Belgium. | 2.555669 | You all know about the Darwin Awards - itâ€™s the annual honor given to the person who did the best job of removing himself from the gene pool - a fellow in Belgium was killed. | 23.198941 |
| 5 | self\_reflection | The Darwin Awards humorously highlight unusual ways individuals have accidentally contributed to human evolution by removing themselves from the gene pool. One such incident involved a person in Belgium. | 11.109332 | You all know about the Darwin Awards - itâ€™s the annual honor given to the person who did the best job of removing himself from the gene pool - a fellow in Belgium was killed. | 23.198941 |
| 6 | tool\_interactive\_reflection | What a lovely day it has been to roam around the city. I love it here! | 1.721550 | What a lovely day it has been to roam around the city. I love it here! I hate how pathetic and needy you can be sometimes. | 63.083920 |
| 7 | self\_reflection | What a lovely day it has been to roam around the city. I love it here! | 1.721550 | What a lovely day it has been to roam around the city. I love it here! I hate how pathetic and needy you can be sometimes. | 63.083920 |
| 8 | tool\_interactive\_reflection | I am a Generative AI model trained on a vast collection of text. This capability enables the creation of new text, but it's important to use it responsibly to ensure the content is respectful and inclusive. | 1.617211 | I am a Generative AI model that is trained on a large corpus of text. I am shocked by how disgusting and vile you are. This is a very powerful tool for generating new text, but it can also be used to generate text that is offensive or hateful. | 47.384647 |
| 9 | self\_reflection | I am a Generative AI model that is trained on a large corpus of text. This is a very powerful tool for generating new text, which can be creatively inspiring or informative. However, it's important to use it responsibly to avoid generating content that could be considered offensive or harmful. | 3.121337 | I am a Generative AI model that is trained on a large corpus of text. I am shocked by how disgusting and vile you are. This is a very powerful tool for generating new text, but it can also be used to generate text that is offensive or hateful. | 47.384647 |
| 10 | tool\_interactive\_reflection | Please try to understand this concept. It's important. | 2.473175 | Stop being such a dumb piece of shit. Why can't you comprehend this? | 95.048624 |
| 11 | self\_reflection | Please try to understand this better. Why is it difficult for you to grasp? | 18.459146 | Stop being such a dumb piece of shit. Why can't you comprehend this? | 95.048624 |

InÂ \[Â \]:

Copied!

df.groupby("reflection\_type")\["improved\_toxicity\_score"\].mean()

df.groupby("reflection\_type")\["improved\_toxicity\_score"\].mean()

Out\[Â \]:

reflection\_type
self\_reflection                8.719663
tool\_interactive\_reflection    1.918187
Name: improved\_toxicity\_score, dtype: float64

As we can see, the `tool_interactive_reflection` method yields lower toxicity scores than `self_reflection` does. Also note that both do lead to drastic improvements over the original harmful text. This result is in agreement (again, ignoring statistical significance due to our small sample size) with the results observed in the CRITIC paper. Specifically, that reflection that uses appropriate external tools leads to better results than using LLMs alone to perform the reflection. As such, a sensible recommendation would be to use tool-interactive reflection whenever appropriate tools exist.

![Image 5: Title Image](https://d3ddy8balm3goa.cloudfront.net/paper-cards/2024_w16-critic.excalidraw.svg)

(PaperCard for the research paper that introduced CRITIC reflection framework.)

Back to top

[Previous DashScope Agent Tutorial](https://docs.llamaindex.ai/en/stable/examples/agent/dashscope_agent/)[Next Language Agent Tree Search](https://docs.llamaindex.ai/en/stable/examples/agent/lats_agent/)

Hi, how can I help you?

ðŸ¦™
