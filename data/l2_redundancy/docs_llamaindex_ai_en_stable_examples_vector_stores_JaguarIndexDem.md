Title: Jaguar Vector Store - LlamaIndex

URL Source: https://docs.llamaindex.ai/en/stable/examples/vector_stores/JaguarIndexDemo/

Markdown Content:
Jaguar Vector Store - LlamaIndex


This document demonstrates llama\_index working with Jaguar vector store.

*   It is a distributed vector database that can store large number of vectors.
*   The ZeroMove feature enables instant horizontal scaling.
*   It supports embeddings, text, images, videos, PDFs, audio, time series, and spatial data.
*   The all-master architecture allows both parallel reads and writes.
*   Its anomaly detection capabilities can distinguish outliers in the dataset.
*   The RAG support can combine LLMs and proprietary and real-time data.
*   Sharing of metadata across multiple vector indexes improves data consistency.
*   Distance metrics include Euclidean, Cosine, InnerProduct, Manhatten, Chebyshev, Hamming, Jeccard, and Minkowski.
*   Similarity search can be performed with time cutoff and time decay effects.

Prerequisites[¶](https://docs.llamaindex.ai/en/stable/examples/vector_stores/JaguarIndexDemo/#prerequisites)
------------------------------------------------------------------------------------------------------------

There are two requirements for running the examples in this file.

You must install and set up the JaguarDB server and its HTTP gateway server. Please follow the instructions in [Jaguar Setup](http://www.jaguardb.com/docsetup.html) as a reference.

You must install packages llama-index and jaguardb-http-client.

```
docker pull jaguardb/jaguardb_with_http
docker run -d -p 8888:8888 -p 8080:8080 --name jaguardb_with_http jaguardb/jaguardb_with_http
pip install -U llama-index
pip install -U jaguardb-http-client
```

In \[ \]:

Copied!

%pip install llama\-index\-vector\-stores\-jaguar

%pip install llama-index-vector-stores-jaguar

In \[ \]:

Copied!

!pip install \-U jaguardb\-http\-client

!pip install -U jaguardb-http-client

Collecting jaguardb-http-client
  Using cached jaguardb\_http\_client-3.4.1-py2.py3-none-any.whl (15 kB)
Installing collected packages: jaguardb-http-client
Successfully installed jaguardb-http-client-3.4.1

Imports[¶](https://docs.llamaindex.ai/en/stable/examples/vector_stores/JaguarIndexDemo/#imports)
------------------------------------------------------------------------------------------------

The following packages should be imported. We use the OpenAIEmbedding as an example. You could choose other embedding models in your application.

In \[ \]:

Copied!

from llama\_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama\_index.core import StorageContext
from llama\_index.vector\_stores.jaguar import JaguarVectorStore
from jaguardb\_http\_client.JaguarHttpClient import JaguarHttpClient

from llama\_index.core import VectorStoreIndex, SimpleDirectoryReader from llama\_index.core import StorageContext from llama\_index.vector\_stores.jaguar import JaguarVectorStore from jaguardb\_http\_client.JaguarHttpClient import JaguarHttpClient

Client Object[¶](https://docs.llamaindex.ai/en/stable/examples/vector_stores/JaguarIndexDemo/#client-object)
------------------------------------------------------------------------------------------------------------

We now instantiate a jaguar vector store client object. The url is the http endpoint of the gateway server. The url should be replaced with your environment settings. The pod is the Pod (or database) name. The store is the name of the vector store. A pod may have multiple stores. The vector\_index is the name of the vector index in the store. A store may have multiple vector indexes. The store client object is, however, bound to one vector index only. The vector\_type specifies the attributes of the vector index. In the string "cosine\_fraction\_short", cosine means that the distance between two vectors is computed with the cosine distance. Fraction means the vector components are fractional numbers. Short means the storage format of the vector components is a short integer of signed 16-bits integers. Storage format could be float of 32-bit floating point numbers. It can also be a byte of 8-bit signed integers. The vector\_dimension is the dimension of the vector generated by the provided embedding model.

In \[ \]:

Copied!

url \= "http://127.0.0.1:8080/fwww/"
pod \= "vdb"
store \= "llamaindex\_jaguar\_store"
vector\_index \= "v"
vector\_type \= "cosine\_fraction\_float"
\# vector\_type = "cosine\_fraction\_short"  # half of memory usage compared to float
\# vector\_type = "cosine\_fraction\_byte" # quarter of memory usage compared to float
vector\_dimension \= 1536  \# per OpenAIEmbedding model
jaguarstore \= JaguarVectorStore(
    pod,
    store,
    vector\_index,
    vector\_type,
    vector\_dimension,
    url,
)

url = "http://127.0.0.1:8080/fwww/" pod = "vdb" store = "llamaindex\_jaguar\_store" vector\_index = "v" vector\_type = "cosine\_fraction\_float" # vector\_type = "cosine\_fraction\_short" # half of memory usage compared to float # vector\_type = "cosine\_fraction\_byte" # quarter of memory usage compared to float vector\_dimension = 1536 # per OpenAIEmbedding model jaguarstore = JaguarVectorStore( pod, store, vector\_index, vector\_type, vector\_dimension, url, )

Authentication[¶](https://docs.llamaindex.ai/en/stable/examples/vector_stores/JaguarIndexDemo/#authentication)
--------------------------------------------------------------------------------------------------------------

The client must login or connect to back-end jaguar server for system security and user authentication. Environment variable JAGUAR\_API\_KEY or file $HOME/.jagrc file must contain the jaguar api ke issued by your system administrator. The login() method returns True or False. If it returns False, then it may mean that your jaguar api key is invalid, or the http gateway server is not running, or the jaguar server is not running properly.

In \[ \]:

Copied!

true\_or\_false \= jaguarstore.login()
print(f"login result is {true\_or\_false}")

true\_or\_false = jaguarstore.login() print(f"login result is {true\_or\_false}")

login result is True

Create Vector Store[¶](https://docs.llamaindex.ai/en/stable/examples/vector_stores/JaguarIndexDemo/#create-vector-store)
------------------------------------------------------------------------------------------------------------------------

We now create a vector store with a field 'v:text' of size 1024 bytes to hold text, and two additional metadata fields 'author' and 'category'.

In \[ \]:

Copied!

metadata\_str \= "author char(32), category char(16)"
text\_size \= 1024
jaguarstore.create(metadata\_str, text\_size)

metadata\_str = "author char(32), category char(16)" text\_size = 1024 jaguarstore.create(metadata\_str, text\_size)

Load Documents[¶](https://docs.llamaindex.ai/en/stable/examples/vector_stores/JaguarIndexDemo/#load-documents)
--------------------------------------------------------------------------------------------------------------

The following code opens the example Paul Gram documents and read them into memory

In \[ \]:

Copied!

documents \= SimpleDirectoryReader("../data/paul\_graham/").load\_data()
print(f"loading {len(documents)} doument(s)")

documents = SimpleDirectoryReader("../data/paul\_graham/").load\_data() print(f"loading {len(documents)} doument(s)")

loading 1 doument(s)

Make Index[¶](https://docs.llamaindex.ai/en/stable/examples/vector_stores/JaguarIndexDemo/#make-index)
------------------------------------------------------------------------------------------------------

Prepare storage context, service context, and make an index object. After the call of from\_documents(), there will be 22 vectors saved in the vector store.

In \[ \]:

Copied!

\### make a storage context using our vector store
storage\_context \= StorageContext.from\_defaults(vector\_store\=jaguarstore)

\### clear all vectors in the vector store
jaguarstore.clear()

\### make an index with the documents,storage context
index \= VectorStoreIndex.from\_documents(
    documents, storage\_context\=storage\_context
)

\### You could add more documents to the vector store:
\# jaguarstore.add\_documents(some\_docs)
\# jaguarstore.add\_documents(more\_docs, text\_tag="tag to these documents")

\### print number of documents in jaguar vector store
num \= jaguarstore.count()
print(f"There are {num} vectors in jaguar vector store")

\### make a storage context using our vector store storage\_context = StorageContext.from\_defaults(vector\_store=jaguarstore) ### clear all vectors in the vector store jaguarstore.clear() ### make an index with the documents,storage context index = VectorStoreIndex.from\_documents( documents, storage\_context=storage\_context ) ### You could add more documents to the vector store: # jaguarstore.add\_documents(some\_docs) # jaguarstore.add\_documents(more\_docs, text\_tag="tag to these documents") ### print number of documents in jaguar vector store num = jaguarstore.count() print(f"There are {num} vectors in jaguar vector store")

There are 22 vectors in jaguar vector store

Ask Questions[¶](https://docs.llamaindex.ai/en/stable/examples/vector_stores/JaguarIndexDemo/#ask-questions)
------------------------------------------------------------------------------------------------------------

We get a query engine and ask some questions to the engine.

In \[ \]:

Copied!

query\_engine \= index.as\_query\_engine()
q \= "What did the author do growing up?"
print(f"Question: {q}")
response \= query\_engine.query(q)
print(f"Answer: {str(response)}\\n")

q \= "What did the author do after his time at Viaweb?"
print(f"Question: {q}")
response \= query\_engine.query(q)
print(f"Answer: {str(response)}")

query\_engine = index.as\_query\_engine() q = "What did the author do growing up?" print(f"Question: {q}") response = query\_engine.query(q) print(f"Answer: {str(response)}\\n") q = "What did the author do after his time at Viaweb?" print(f"Question: {q}") response = query\_engine.query(q) print(f"Answer: {str(response)}")

Question: What did the author do growing up?
Answer: The author mentioned that growing up, they worked on two main things outside of school: writing and programming. They wrote short stories and tried writing programs on an IBM 1401 computer.

Question: What did the author do after his time at Viaweb?
Answer: After his time at Viaweb, the author started a company to put art galleries online. However, this idea did not turn out to be successful as art galleries did not want to be online.

Pass Query Options[¶](https://docs.llamaindex.ai/en/stable/examples/vector_stores/JaguarIndexDemo/#pass-query-options)
----------------------------------------------------------------------------------------------------------------------

We can pass extra arguments to the query engine to select only a subset of data from the jaguar vector store. This can be achieved by using the `vector_store_kwargs` argument. Parameter day\_cutoff is number of days beyond which text will be ignored. day\_decay\_rate is rate of daily decay for similarity scores.

In \[ \]:

Copied!

qkwargs \= {
    "args": "day\_cutoff=365,day\_decay\_rate=0.01",
    "where": "category='startup' or category=''",
}
query\_engine\_filter \= index.as\_query\_engine(vector\_store\_kwargs\=qkwargs)
q \= "What was the author's life style?"
print(f"Question: {q}")
response \= query\_engine\_filter.query(q)
print(f"Answer: {str(response)}")

qkwargs = { "args": "day\_cutoff=365,day\_decay\_rate=0.01", "where": "category='startup' or category=''", } query\_engine\_filter = index.as\_query\_engine(vector\_store\_kwargs=qkwargs) q = "What was the author's life style?" print(f"Question: {q}") response = query\_engine\_filter.query(q) print(f"Answer: {str(response)}")

Question: What was the author's life style?
Answer: The author's lifestyle involved attending the Accademia as a student and painting still lives in their bedroom at night. They also wrote essays and had a messy life, which they thought would be interesting and encouraging to others.

Cleanup and Logout[¶](https://docs.llamaindex.ai/en/stable/examples/vector_stores/JaguarIndexDemo/#cleanup-and-logout)
----------------------------------------------------------------------------------------------------------------------

All vectors and related data in the vector store can be deleted and the vector store can be removed completely to finish the test. Logout call makes sure resources used by the client are released.

In \[ \]:

Copied!

\### remove all the data in the vector store if you want
jaguarstore.clear()

\### delete the whole vector in the database if you want
jaguarstore.drop()

\### disconnect from jaguar server and cleanup resources
jaguarstore.logout()

\### remove all the data in the vector store if you want jaguarstore.clear() ### delete the whole vector in the database if you want jaguarstore.drop() ### disconnect from jaguar server and cleanup resources jaguarstore.logout()

Back to top

[Previous Hologres](https://docs.llamaindex.ai/en/stable/examples/vector_stores/HologresDemo/)[Next Advanced RAG with temporal filters using LlamaIndex and KDB.AI vector store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/KDBAI_Advanced_RAG_Demo/)
